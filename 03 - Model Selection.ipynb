{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1f3789-4adf-4601-8130-14b82be440cb",
   "metadata": {},
   "source": [
    "# Notebook 3: Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411bf36d-56ce-47f4-92e6-31fa585a3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import  mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "import math\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import keras_tuner\n",
    "\n",
    "from scipy.special import logit, expit\n",
    "from utils import logit_func, expit_func\n",
    "\n",
    "import optuna\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "# Set max column width to None.\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Import cleaned dataset.\n",
    "df = pd.read_csv('df_filtered.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d5813-1246-4c2e-89bc-c76041af5fcb",
   "metadata": {},
   "source": [
    "In this notebook, we evaluate several regression models for predicting pre-snap win probability, comparing linear, tree-based, and neural approaches. \n",
    "\n",
    "We clip the range of target values to avoid undefined errors at 0 and 1. We then logit-transform our target to stabilize regression and avoid compression at the extremes of 0 and 1. The subsequent inverse transform allows predictions to be returned as probabilities.\n",
    "\n",
    "To keep preprocessing reproducible and portable across experiments and deployment, this transformation is encapsulated in logit_func and expit_func within utils.py, and then wrapped in TransformedTargetRegressor. These ensure consistent handling of edge cases [0, 1] during both training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194e9c41-f40e-4415-a9e5-742448287abd",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7351f45b-ee37-4861-b599-c9edd579a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_encode = [\n",
    "    'down'\n",
    "]\n",
    "\n",
    "features_to_scale = [\n",
    "    'yardline_100_home',\n",
    "    'ydstogo',\n",
    "    'home_score_differential',\n",
    "    'home_spread_line'\n",
    "]\n",
    "\n",
    "passthrough_features = [\n",
    "    'home_pos', \n",
    "    'time_weight', \n",
    "    'home_timeouts_remaining',\n",
    "    'away_timeouts_remaining']\n",
    "\n",
    "target = ['vegas_home_wp']\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('encoder', OneHotEncoder(drop='first', sparse_output=False), features_to_encode),\n",
    "        ('scaler', StandardScaler(), features_to_scale),\n",
    "        ('noop', FunctionTransformer(validate=False), passthrough_features)  \n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "\n",
    "X = df.drop('vegas_home_wp', axis = 1)\n",
    "y = df['vegas_home_wp']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa92a6-08ac-4165-9126-280e89924abd",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Ridge/Lasso: These are linear models; regularization tests whether a simple linear approximation can capture win probability dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221eeef-3828-4c8d-bf57-6f0a38385c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge.\n",
    "\n",
    "\n",
    "ridge_params = {\n",
    "    'model__alpha': np.logspace(-3, 3, 20),\n",
    "    'model__fit_intercept': [True, False],\n",
    "    'model__max_iter': [1000, 5000, 10000],\n",
    "    'model__tol': [1e-4, 1e-3, 1e-2],\n",
    "    'model__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga'],\n",
    "    # 'model__solver': ['auto', 'svd', 'cholesky', 'sag', 'saga'],\n",
    "    # 'model__positive': [True, False],\n",
    "    'model__copy_X': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "ridge_model = Ridge(random_state = 42)\n",
    "\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', ridge_model)\n",
    "])\n",
    "\n",
    "ridge_search = RandomizedSearchCV(\n",
    "        ridge_pipeline,\n",
    "        ridge_params,\n",
    "        n_iter=100,\n",
    "        cv=3,\n",
    "        scoring = 'neg_mean_squared_error',\n",
    "        verbose = 3,\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "# Transformed target regressor with logit/expit transformation.\n",
    "ridge_model = TransformedTargetRegressor(\n",
    "    regressor = ridge_search,\n",
    "    func = logit_func,\n",
    "    inverse_func = expit_func\n",
    ")\n",
    "\n",
    "\n",
    "ridge_model.fit(X, y)\n",
    "print(f'Ridge best score: {-ridge_model.regressor_.best_score_}')\n",
    "print(f'Ridge best params: {ridge_model.regressor_.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb60144-7efd-4de7-8f57-2ac5f61e8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso.\n",
    "\n",
    "lasso_params = {\n",
    "    'model__alpha': np.logspace(-4, 1, 20),\n",
    "    'model__max_iter': [1000, 5000, 10000],\n",
    "    'model__fit_intercept': [True, False],\n",
    "    'model__tol': [1e-4, 1e-3, 1e-2],\n",
    "    'model__selection': ['cyclic', 'random'],\n",
    "    'model__positive': [True, False],\n",
    "    'model__warm_start': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "lasso_model = Lasso(random_state = 42)\n",
    "\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', lasso_model)\n",
    "])\n",
    "\n",
    "\n",
    "lasso_search = RandomizedSearchCV(\n",
    "        lasso_pipeline,\n",
    "        lasso_params,\n",
    "        n_iter=100,\n",
    "        cv=3,\n",
    "        scoring = 'neg_mean_squared_error',\n",
    "        verbose = 3, \n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "lasso_model = TransformedTargetRegressor(\n",
    "    regressor = lasso_search,\n",
    "    func = logit_func,\n",
    "    inverse_func = expit_func\n",
    ")\n",
    "\n",
    "\n",
    "lasso_model.fit(X, y)\n",
    "print(f'Lasso best score: {-lasso_model.regressor_.best_score_}')\n",
    "print(f'Lasso best params: {lasso_model.regressor_.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae599c-a4ba-4d83-a29e-8a861226112a",
   "metadata": {},
   "source": [
    "Random Forest: Tree-based ensemble that captures nonlinearities without heavy tuning, but less efficient than boosting methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109aa18-f021-4cf7-bb01-6678046faa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest.\n",
    "\n",
    "rf_params = {\n",
    "            'model__n_estimators': [50, 100, 200, 500],\n",
    "            'model__max_depth': [None, 10, 20, 30, 50],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4],\n",
    "            'model__max_features': ['sqrt', 'log2'],\n",
    "            'model__bootstrap': [True, False]\n",
    "        }\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "        rf_pipeline,\n",
    "        rf_params,\n",
    "        n_iter=100,\n",
    "        cv=3,\n",
    "        scoring = 'neg_mean_squared_error',\n",
    "        verbose = 3,\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "rf_model = TransformedTargetRegressor(\n",
    "    regressor = rf_search,\n",
    "    func = logit_func,\n",
    "    inverse_func = expit_func\n",
    ")\n",
    "\n",
    "\n",
    "rf_model.fit(X, y)\n",
    "print(f'Random forest best score: {-rf_model.regressor_.best_score_}')\n",
    "print(f'Random forest best params: {rf_model.regressor_.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb9537-d31f-48fc-8aca-b6d5bc112899",
   "metadata": {},
   "source": [
    "XGBoost: Gradient boosting, strong candidate for tabular data, capable of modeling interactions while controlling overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f39753-2f40-4466-866a-6d41245dc95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost.\n",
    "\n",
    "xgb_params = {\n",
    "    'model__n_estimators': [100, 300, 500],\n",
    "    'model__learning_rate': [.01, .05, .1, .3],\n",
    "    'model__subsample': [.5, .7, 1],\n",
    "    'model__colsample_bytree': [.5, .7, 1],\n",
    "    'model__reg_alpha': np.logspace(-4, 4, 20),\n",
    "    'model__reg_lambda': np.logspace(-4, 4, 20),\n",
    "    'model__gamma': [0, .1, .3, 1],\n",
    "    'model__max_depth': [3, 5, 7, 10]\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor(random_state = 42)\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', xgb_model)\n",
    "])\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "        xgb_pipeline,\n",
    "        xgb_params,\n",
    "        n_iter=100,\n",
    "        cv=3,\n",
    "        scoring = 'neg_mean_squared_error',\n",
    "        verbose = 3, \n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "xgb_model = TransformedTargetRegressor(\n",
    "    regressor = xgb_search,\n",
    "    func = logit_func,\n",
    "    inverse_func = expit_func\n",
    ")\n",
    "\n",
    "\n",
    "xgb_model.fit(X, y)\n",
    "print(f'XGBoost best score: {-xgb_model.regressor_.best_score_}')\n",
    "print(f'XGBoost best params: {xgb_model.regressor_.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb79b00-8860-43da-8462-4756336a436d",
   "metadata": {},
   "source": [
    "Neural network: Explores deep learning's ability to model complex relationships, through it is less interpretable.\n",
    "\n",
    "GridSearchCV and RandomizedSearchCV do not allow testing with different numbers of units among varying numbers of layers. Keras_tuner.RandomSearch will test 2, 3, 4, and 5 layers, with varying numbers of units in each layer. \n",
    "\n",
    "Keras_tuner is not compatible with our pipeline as it does not employ the .fit() method. Therefore we transform X_train and X_test manually, and feed the transformed results into the tuner.\n",
    "\n",
    "We perform 100 runs with 10 epochs each, and then evaluate the top 5 models. We also repeat this process several times to ensure consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b443a8-f6c3-4555-ba2e-7f997aa83257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset X and y.\n",
    "X = df.drop('vegas_home_wp', axis = 1)\n",
    "y = df['vegas_home_wp']\n",
    "\n",
    "# Explicitly define logit function.\n",
    "def logit_func(y):\n",
    "    tol = 1e-5\n",
    "    return logit(np.clip(y, tol, 1-tol))\n",
    "def expit_func(y):\n",
    "    return expit(y)\n",
    "\n",
    "# Logit-transform target.\n",
    "y = logit_func(y)\n",
    "\n",
    "# Train-test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = .2,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Transform data.\n",
    "X_train_transformed = preprocessing.fit_transform(X_train)\n",
    "X_test_transformed = preprocessing.transform(X_test)\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "# Create function to build keras models.\n",
    "def build_model(hp):\n",
    "    keras_model = Sequential()\n",
    "    \n",
    "    num_layers = hp.Int('num_layers', \n",
    "                        min_value = 2, \n",
    "                        max_value = 5)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        keras_model.add(Dense(hp.Choice(f'units_{i}', [16, 32, 64, 128, 256, 512, 1024]),\n",
    "                       activation = 'relu'))\n",
    "    keras_model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "    keras_model.compile(optimizer = 'adam', \n",
    "                  loss = 'mse', \n",
    "                  metrics = ['mse'])\n",
    "    return keras_model\n",
    "\n",
    "# Instantiate tuner with build function.\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective = 'mse',\n",
    "    max_trials = 100,\n",
    "    overwrite = True\n",
    ")\n",
    "\n",
    "# Use tuner to search.\n",
    "tuner.search(X_train_transformed, \n",
    "             y_train, \n",
    "             epochs = 10, \n",
    "             validation_data = (X_test_transformed, y_test))\n",
    "\n",
    "# Top models.\n",
    "top_models = tuner.get_best_models(num_models=5)\n",
    "\n",
    "# View summary of top models.\n",
    "for i, model in enumerate(top_models):\n",
    "    model.build(input_shape = (None, X_train_transformed.shape[1]))\n",
    "    loss, acc = model.evaluate(X_test_transformed, y_test, verbose = 0)\n",
    "\n",
    "    print(f\"\\nModel {i+1} Summary: (Accuracy: {acc:.4f})\")\n",
    "    print(f'test: {loss}')\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723622b-d07d-4c0f-a8ba-e109d72bfcef",
   "metadata": {},
   "source": [
    "After several iterations, the top performing sequential neural network configuration was consistently 128/512/128/512/1. \n",
    "\n",
    "We train a neural network with the optimal hyperparameters over 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf24231-0dcb-4b27-998d-6a2a2e533fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model.\n",
    "model = Sequential()\n",
    "\n",
    "# Set layers.\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "# Train model.\n",
    "model.fit(X_train_transformed, \n",
    "          y_train, \n",
    "          batch_size = 32, \n",
    "          epochs = 100, \n",
    "          validation_data=(X_test_transformed, y_test))\n",
    "\n",
    "# View summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f49356-54dc-48ed-8449-4b45f7a39613",
   "metadata": {},
   "source": [
    "### Best MSEs:\n",
    "\n",
    "Ridge: 1.085    \n",
    "Lasso: 1.085    \n",
    "Random Forest: .047   \n",
    "XGBoost: .021    \n",
    "Neural Network: .039    \n",
    "\n",
    "The linear models (Ridge, Lasso) performed poorly, suggesting the relationship between features and win probability is nonlinear. Random forest and neural network improved fit substantially, but XGBoost provided the lowest MSE while remaining efficient and stable across folds. As a result, we select XGBoost as our production model.\n",
    "\n",
    "We select Optuna for refinement because it explores parameter space more efficiently than grid or random search. This reduced training time while improving performance over baseline RandomizedSearchCV results.\n",
    "\n",
    "\n",
    "XGBoost best hyperparameters from randomized search:\n",
    " - 'model__colsample_bytree': 0.7,\n",
    " - 'model__gamma': 0, \n",
    " - 'model__learning_rate': 0.1, \n",
    " - 'model__max_depth': 10, \n",
    " - 'model__n_estimators': 300, \n",
    " - 'model__reg_alpha': 1.623776739188721, \n",
    " - 'model__reg_lambda': 11.288378916846883, \n",
    " - 'model__subsample': 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f117e71-1f76-4b49-90eb-2df00eb2ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Subset X and y.\n",
    "X = df.drop('vegas_home_wp', axis = 1)\n",
    "y = df['vegas_home_wp']\n",
    "\n",
    "# Train-test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = .2,\n",
    "                                                    random_state = 42)\n",
    "# Define objective.\n",
    "def objective(trial):\n",
    "    # Define hyperparameter space.\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', .01, .3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 20.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 20.0),\n",
    "        'min_child_weight':trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'gamma': trial.suggest_int('gamma', 0, 5),\n",
    "        'random_state': 42\n",
    "        \n",
    "    }\n",
    "   \n",
    "    # Instantiate model.\n",
    "    xgb_model = XGBRegressor(random_state = 42)\n",
    "\n",
    "    # Wrap model in pipeline.\n",
    "    model_pipe = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('model', xgb_model)\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    # Wrap pipeline in transformed target regressor.\n",
    "    final_model = TransformedTargetRegressor(\n",
    "        regressor = model_pipe,\n",
    "        func = logit_func,\n",
    "        inverse_func = expit_func\n",
    "        )\n",
    "\n",
    "    # Extract parameters from dictionary.\n",
    "    final_model.set_params(\n",
    "        **{f'regressor__model__{key}':value for key, value in params.items()})\n",
    "\n",
    "    # Set scoring to cross validation with negative MSE.\n",
    "    score = cross_val_score(\n",
    "        model_pipe,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=3,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    return float(score.mean())\n",
    "\n",
    "# Set save path.\n",
    "cwd = os.getcwd()\n",
    "db_path = f\"sqlite:///{cwd}/xgb_tuning_03.db\"\n",
    "\n",
    "# Random seed in sampler for reproducibility.\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "\n",
    "# Instantiate study with sampler.\n",
    "study = optuna.create_study(\n",
    "    study_name=\"xgb_tuning_03\",\n",
    "    direction=\"maximize\",\n",
    "    storage=db_path,\n",
    "    load_if_exists=True,\n",
    "    sampler = sampler\n",
    "    # skip_if_exists=True\n",
    ")\n",
    "\n",
    "# Optimize study, 500 iterations.\n",
    "study.optimize(objective, n_trials = 500)\n",
    "\n",
    "# View best MSE.\n",
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75050327-c6c5-4010-95ca-d68c4fa5827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View best hyperparameters.\n",
    "best_params = study.best_params\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4a7dd-fcdf-4801-b2a7-54f8e2b755b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly define best hyperparameters.\n",
    "best_params = {\n",
    "    'n_estimators': 387,\n",
    "    'max_depth': 15,\n",
    "    'learning_rate': 0.06537652719126918,\n",
    "    'subsample': 0.9286366815201457,\n",
    "    'colsample_bytree': 0.9793042383096315,\n",
    "    'colsample_bylevel': 0.9450262365116351,\n",
    "    'colsample_bynode': 0.9847881824040057,\n",
    "    'reg_alpha': 0.003750294655665686,\n",
    "    'reg_lambda': 16.297997163154285,\n",
    "    'min_child_weight': 7,\n",
    "    'gamma': 0}\n",
    "\n",
    "# Print list copy-paste friendly list of hyperparameters.\n",
    "for k, v in best_params.items():\n",
    "    print(f'{k} = {v},')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363bfb68-a31d-479d-b632-819370e3a01b",
   "metadata": {},
   "source": [
    "Optuna gave us these hyperparameters: \n",
    "\n",
    " - n_estimators = 387,  \n",
    " - max_depth = 15,  \n",
    " - learning_rate = 0.06537652719126918,  \n",
    " - subsample = 0.9286366815201457,  \n",
    " - colsample_bytree = 0.9793042383096315,  \n",
    " - colsample_bylevel = 0.9450262365116351,  \n",
    " - colsample_bynode = 0.9847881824040057,  \n",
    " - reg_alpha = 0.003750294655665686,  \n",
    " - reg_lambda = 16.297997163154285,  \n",
    " - min_child_weight = 7,  \n",
    " - gamma = 0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d6fd047-85be-4dbc-a1e7-7819974266ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model R²: 0.9971068400060967\n",
      "Model MSE: 0.00028795634696128486\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('vegas_home_wp', axis = 1)\n",
    "y = df['vegas_home_wp']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = .2,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Instantiate XGBoost model with optimal hyperparameters.\n",
    "xgb_refined_model = XGBRegressor(\n",
    "    n_estimators = 387,\n",
    "    max_depth = 15,\n",
    "    learning_rate = 0.06537652719126918,\n",
    "    subsample = 0.9286366815201457,\n",
    "    colsample_bytree = 0.9793042383096315,\n",
    "    colsample_bylevel = 0.9450262365116351,\n",
    "    colsample_bynode = 0.9847881824040057,\n",
    "    reg_alpha = 0.003750294655665686,\n",
    "    reg_lambda = 16.297997163154285,\n",
    "    min_child_weight = 7,\n",
    "    gamma = 0,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# Model wrapped in pipeline.\n",
    "model_pipe = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', xgb_refined_model)\n",
    "])\n",
    "\n",
    "\n",
    "# Wrap pipeline in transformed target regressor.\n",
    "final_model = TransformedTargetRegressor(\n",
    "    regressor = model_pipe,\n",
    "    func = logit_func,\n",
    "    inverse_func = expit_func\n",
    ")\n",
    "\n",
    "\n",
    "# Fit model.\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict.\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# View results.\n",
    "print(f'Model R²: {r2_score(y_test, y_pred)}')\n",
    "print(f'Model MSE: {mean_squared_error(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41871aca-31bc-47ca-8bc9-26a85f3db54c",
   "metadata": {},
   "source": [
    "Our final model resulted in an R² of .997, meaning the model explains over 99% of the variation in sportsbook win probability, along with an inverse-transformed MSE of .0003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d08e0362-c448-44f3-97f1-945a2c41275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model and combined dataframe.\n",
    "joblib.dump(final_model, 'final_model.pkl', compress=3)\n",
    "\n",
    "df_preout = pd.concat([X, y], axis = 1)\n",
    "\n",
    "y_pred_full = final_model.predict(X)\n",
    "\n",
    "df_out = pd.concat([df_preout, pd.Series(y_pred_full, index = df_preout.index, name = 'y_pred')], axis = 1)\n",
    "df_out.to_csv('df_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c3621-86ca-4759-b9b4-f37ba407a78f",
   "metadata": {},
   "source": [
    "## Model Selection Summary\n",
    "\n",
    " - Linear models (Ridge, Lasso) underfit, confirming the need for nonlinear methods.\n",
    " - Tree-based models (Random Forest, XGBoost) captured interactions effectively; XGBoost achieved the best generalization.\n",
    " - Neural networks performed competitively.\n",
    " - Optuna hyperparameter tuning improved efficiency and final model performance.\n",
    "\n",
    "We select XGBoost as the final model due to its stability and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
